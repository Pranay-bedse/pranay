{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0052de9b",
   "metadata": {},
   "source": [
    "## Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2cf95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ee7d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.0.249-py3-none-any.whl (1.4 MB)\n",
      "                                              0.0/1.4 MB ? eta -:--:--\n",
      "     -------                                  0.3/1.4 MB 7.9 MB/s eta 0:00:01\n",
      "     -----------------                        0.6/1.4 MB 7.5 MB/s eta 0:00:01\n",
      "     ----------------------------             1.0/1.4 MB 7.8 MB/s eta 0:00:01\n",
      "     -------------------------------------    1.3/1.4 MB 7.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.4/1.4 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.4/1.4 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.4/1.4 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.4/1.4 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.4/1.4 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.4/1.4 MB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (3.8.3)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
      "  Downloading langsmith-0.0.16-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "                                              0.0/90.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 90.0/90.0 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting pydantic<2,>=1 (from langchain)\n",
      "  Downloading pydantic-1.10.12-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "                                              0.0/2.1 MB ? eta -:--:--\n",
      "     ------                                   0.3/2.1 MB 10.2 MB/s eta 0:00:01\n",
      "     -----------                              0.6/2.1 MB 7.5 MB/s eta 0:00:01\n",
      "     ------------------                       1.0/2.1 MB 7.5 MB/s eta 0:00:01\n",
      "     ------------------                       1.0/2.1 MB 7.0 MB/s eta 0:00:01\n",
      "     -------------------                      1.0/2.1 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------------                   1.2/2.1 MB 4.4 MB/s eta 0:00:01\n",
      "     ----------------------------             1.5/2.1 MB 4.9 MB/s eta 0:00:01\n",
      "     --------------------------------         1.7/2.1 MB 5.0 MB/s eta 0:00:01\n",
      "     -------------------------------------    2.0/2.1 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (2.29.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "                                              0.0/49.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 49.4/49.4 kB ? eta 0:00:00\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Installing collected packages: typing-inspect, pydantic, marshmallow, openapi-schema-pydantic, langsmith, dataclasses-json, langchain\n",
      "Successfully installed dataclasses-json-0.5.14 langchain-0.0.249 langsmith-0.0.16 marshmallow-3.20.1 openapi-schema-pydantic-1.2.4 pydantic-1.10.12 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7434c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6641e30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-3.14.0-py3-none-any.whl (269 kB)\n",
      "                                              0.0/269.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 269.8/269.8 kB 8.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-3.14.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pypdf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433ac20",
   "metadata": {},
   "source": [
    "# Accessing information from pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49074bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "#loader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d03abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Let's load the document.\n",
    "#pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a29d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Let see the length of page.\n",
    "#len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc764631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page = pages[0]\n",
    "\n",
    "# #Having a look at the content of the page.\n",
    "# print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3b57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2698b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To see from where data comes from\n",
    "# page.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b50e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6bb60a8",
   "metadata": {},
   "source": [
    "# Accessing the infomation from youtube video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19390ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "\n",
    "# Whisper will convert the audio into the text\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "\n",
    "# Youtube audio loader which load audio part of youtube audio\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "284ab4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url=\"https://www.youtube.com/watch?v=jGwO_UgTS7I\"\n",
    "# save_dir=\"docs/youtube/\"\n",
    "# loader = GenericLoader(\n",
    "#     YoutubeAudioLoader([url],save_dir),\n",
    "#     OpenAIWhisperParser()\n",
    "# )\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ac10d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Having a look at the page content\n",
    "# docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd7966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac5436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f2102e7",
   "metadata": {},
   "source": [
    "# Accessing the information from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ba78ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# loader = WebBaseLoader(\"https://github.com/basecamp/handbook/blob/master/37signals-is-you.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cbef1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64691d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f76b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99a8ed4b",
   "metadata": {},
   "source": [
    "## Notion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa85b4",
   "metadata": {},
   "source": [
    "Follow steps [here](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/notion) for an example Notion site such as [this one](https://yolospace.notion.site/Blendle-s-Employee-Handbook-e31bff7da17346ee99f531087d8b133f):\n",
    "\n",
    "* Duplicate the page into your own Notion space and export as `Markdown / CSV`.\n",
    "* Unzip it and save it as a folder that contains the markdown file for the Notion page.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "677d17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import NotionDirectoryLoader\n",
    "# loader = NotionDirectoryLoader(\"docs/Notion_DB\")\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0c9eee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m200\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# print(docs[0].page_content[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec524d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f3835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ea5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdef3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732359b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b688443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb3fd0c6",
   "metadata": {},
   "source": [
    "## Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c639b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00707b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edf1f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size =26\n",
    "\n",
    "# Number of overlap character can take.\n",
    "chunk_overlap = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3cdee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0451c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why doesn't this split the string below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a41cdbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cce3b54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aee788fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53c4c6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99318b",
   "metadata": {},
   "source": [
    "#### Ok, this splits the string but we have an overlap specified as 5, but it looks like 3? (try an even number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "443048f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d413d4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c123438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m n o p q r s t u v w x y z']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a7935",
   "metadata": {},
   "source": [
    "#### So here c splitter not spliting the words because it will only make spliting after new word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b180bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator = ' '\n",
    ")\n",
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10a778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7dbe2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60541707",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "293e49d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "396082b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0,\n",
    "    separator = ' '\n",
    ")\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81199a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\\'s are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also,',\n",
       " 'have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2de1f598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\",\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84e995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d53caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66077aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related\",\n",
       " '. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns',\n",
       " '. Carriage returns are the \"backslash n\" you see embedded in this string',\n",
       " '. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80013b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related.\",\n",
       " 'For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns.',\n",
       " 'Carriage returns are the \"backslash n\" you see embedded in this string.',\n",
       " 'Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a56520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9e224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef12df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8178488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# loader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
    "# pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0df737c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84d888ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5512825d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8380cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee63676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302b928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "decd9432",
   "metadata": {},
   "source": [
    "## Token splitting\n",
    "\n",
    "We can also split on token count explicity, if we want.\n",
    "\n",
    "This can be useful because LLMs often have context windows designated in tokens.\n",
    "\n",
    "Tokens are often ~4 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fe02094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a7793e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ee635a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"foo bar bazzyfoo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0b7a36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo', ' bar', ' b', 'az', 'zy', 'foo']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d68db920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo', ' bar', ' b', 'az', 'zy', 'foo']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad6b1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26311edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a6647ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c2eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc3227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69ba4a21",
   "metadata": {},
   "source": [
    "## Context aware splitting\n",
    "\n",
    "Chunking aims to keep text with common context together.\n",
    "\n",
    "A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting.\n",
    "\n",
    "We can use `MarkdownHeaderTextSplitter` to preserve header metadata in our chunks, as show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f1e433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ce43aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_document = \"\"\"# Title\\n\\n \\\n",
    "## Chapter 1\\n\\n \\\n",
    "Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n",
    "### Section \\n\\n \\\n",
    "Hi this is Lance \\n\\n \n",
    "## Chapter 2\\n\\n \\\n",
    "Hi this is Molly\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f5b3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fb81837",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5546ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_header_splits = markdown_splitter.split_text(markdown_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9352b2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Hi this is Jim  \\nHi this is Joe', metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1'})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a452543a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Hi this is Lance', metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1', 'Header 3': 'Section'})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d66b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8dacb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca570f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a060aa92",
   "metadata": {},
   "source": [
    "# Vectorstores and Embeddings\n",
    "\n",
    "Recall the overall workflow for retrieval augmented generation (RAG):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82afe6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a355f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# # Load PDF\n",
    "# loaders = [\n",
    "#     # Duplicate documents on purpose - messy data\n",
    "#     PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
    "#     PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
    "#     PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture02.pdf\"),\n",
    "#     PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\")\n",
    "# ]\n",
    "# docs = []\n",
    "# for loader in loaders:\n",
    "#     docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d8e04bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba4cd8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1562e778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae88a1",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Let's take our splits and embed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9acdf94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting embeddings\n",
      "  Downloading embeddings-0.0.8-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from embeddings) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from embeddings) (2.29.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from embeddings) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->embeddings) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->embeddings) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->embeddings) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->embeddings) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm->embeddings) (0.4.6)\n",
      "Installing collected packages: embeddings\n",
      "Successfully installed embeddings-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a21bbcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement OpenAIEmbeddings (from versions: none)\n",
      "ERROR: No matching distribution found for OpenAIEmbeddings\n"
     ]
    }
   ],
   "source": [
    "pip install OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aad6408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "#embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2b179068",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "74e285e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding1 = embedding.embed_query(sentence1)\n",
    "# embedding2 = embedding.embed_query(sentence2)\n",
    "# embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8ec7650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "89aeaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bfdeb366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.dot(embedding1, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "52bc0d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.dot(embedding2, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1384a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65874837",
   "metadata": {},
   "source": [
    "## Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcdc5129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for chroma-hnswlib (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [5 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'hnswlib' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for chroma-hnswlib\n",
      "ERROR: Could not build wheels for chroma-hnswlib, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading chromadb-0.4.4-py3-none-any.whl (402 kB)\n",
      "                                              0.0/402.6 kB ? eta -:--:--\n",
      "     -------------------                    204.8/402.6 kB 6.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 402.6/402.6 kB 6.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from chromadb) (2.29.0)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.9 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from chromadb) (1.10.12)\n",
      "Collecting chroma-hnswlib==0.7.2 (from chromadb)\n",
      "  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
      "                                              0.0/58.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.4/58.4 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
      "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "                                              0.0/59.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.5/59.5 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from chromadb) (1.24.3)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from chromadb) (4.6.3)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
      "  Downloading pulsar_client-3.2.0-cp311-cp311-win_amd64.whl (3.4 MB)\n",
      "                                              0.0/3.4 MB ? eta -:--:--\n",
      "     ---                                      0.3/3.4 MB 6.3 MB/s eta 0:00:01\n",
      "     -------                                  0.6/3.4 MB 7.6 MB/s eta 0:00:01\n",
      "     ----------                               0.9/3.4 MB 7.4 MB/s eta 0:00:01\n",
      "     --------------                           1.2/3.4 MB 7.9 MB/s eta 0:00:01\n",
      "     ------------------                       1.6/3.4 MB 7.7 MB/s eta 0:00:01\n",
      "     ----------------------                   1.9/3.4 MB 7.5 MB/s eta 0:00:01\n",
      "     ---------------------------              2.3/3.4 MB 7.8 MB/s eta 0:00:01\n",
      "     -------------------------------          2.7/3.4 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------        2.8/3.4 MB 7.2 MB/s eta 0:00:01\n",
      "     ------------------------------------     3.1/3.4 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.4/3.4 MB 7.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.4/3.4 MB 6.6 MB/s eta 0:00:00\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.15.1-cp311-cp311-win_amd64.whl (6.7 MB)\n",
      "                                              0.0/6.7 MB ? eta -:--:--\n",
      "     -                                        0.3/6.7 MB 9.6 MB/s eta 0:00:01\n",
      "     ---                                      0.6/6.7 MB 7.7 MB/s eta 0:00:01\n",
      "     -----                                    0.9/6.7 MB 7.0 MB/s eta 0:00:01\n",
      "     -------                                  1.2/6.7 MB 7.8 MB/s eta 0:00:01\n",
      "     --------                                 1.5/6.7 MB 7.4 MB/s eta 0:00:01\n",
      "     -----------                              1.9/6.7 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------                             2.2/6.7 MB 7.7 MB/s eta 0:00:01\n",
      "     --------------                           2.5/6.7 MB 7.6 MB/s eta 0:00:01\n",
      "     -----------------                        2.9/6.7 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------                       3.2/6.7 MB 7.5 MB/s eta 0:00:01\n",
      "     --------------------                     3.5/6.7 MB 7.7 MB/s eta 0:00:01\n",
      "     --------------------                     3.5/6.7 MB 7.7 MB/s eta 0:00:01\n",
      "     ---------------------                    3.7/6.7 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------                 4.1/6.7 MB 6.8 MB/s eta 0:00:01\n",
      "     --------------------------               4.5/6.7 MB 6.8 MB/s eta 0:00:01\n",
      "     ----------------------------             4.9/6.7 MB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------------          5.3/6.7 MB 7.0 MB/s eta 0:00:01\n",
      "     --------------------------------         5.4/6.7 MB 6.8 MB/s eta 0:00:01\n",
      "     ----------------------------------       5.8/6.7 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------------------------------     6.1/6.7 MB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------------------    6.3/6.7 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.6/6.7 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.7/6.7 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.7/6.7 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.7/6.7 MB 6.2 MB/s eta 0:00:00\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-win_amd64.whl (3.5 MB)\n",
      "                                              0.0/3.5 MB ? eta -:--:--\n",
      "     ---                                      0.3/3.5 MB 5.7 MB/s eta 0:00:01\n",
      "     ------                                   0.6/3.5 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------                                0.8/3.5 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------                             1.1/3.5 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------                          1.4/3.5 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------                      1.7/3.5 MB 5.9 MB/s eta 0:00:01\n",
      "     ----------------------                   1.9/3.5 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------                2.2/3.5 MB 6.1 MB/s eta 0:00:01\n",
      "     ----------------------------             2.5/3.5 MB 6.3 MB/s eta 0:00:01\n",
      "     -------------------------------          2.7/3.5 MB 6.3 MB/s eta 0:00:01\n",
      "     ----------------------------------       3.0/3.5 MB 6.2 MB/s eta 0:00:01\n",
      "     --------------------------------------   3.3/3.5 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.5/3.5 MB 6.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.5/3.5 MB 5.8 MB/s eta 0:00:00\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "                                              0.0/67.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.3/67.3 kB 3.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from chromadb) (4.65.0)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.0.0-py3-none-any.whl (31 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "                                              0.0/67.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.0/67.0 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "                                              0.0/46.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.0/46.0 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.0)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-4.23.4-cp310-abi3-win_amd64.whl (422 kB)\n",
      "                                              0.0/422.5 kB ? eta -:--:--\n",
      "     --------------------                   225.3/422.5 kB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------------         337.9/422.5 kB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------         337.9/422.5 kB 5.2 MB/s eta 0:00:01\n",
      "     -------------------------------------  419.8/422.5 kB 2.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 422.5/422.5 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: python-dateutil>2.1 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from pulsar-client>=3.1.0->chromadb) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.28->chromadb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.28->chromadb) (1.26.16)\n",
      "Requirement already satisfied: colorama in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.65.0->chromadb) (0.4.6)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.0.4)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "                                              0.0/58.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.0-cp311-cp311-win_amd64.whl (142 kB)\n",
      "                                              0.0/142.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 142.7/142.7 kB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.19.0-cp37-abi3-win_amd64.whl (270 kB)\n",
      "                                              0.0/270.9 kB ? eta -:--:--\n",
      "     -------------------------------------  266.2/270.9 kB 8.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 270.9/270.9 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-11.0.3-cp311-cp311-win_amd64.whl (124 kB)\n",
      "                                              0.0/124.7 kB ? eta -:--:--\n",
      "     -------------------------------------  122.9/124.7 kB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 124.7/124.7 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.5.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "                                              0.0/86.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.8/86.8 kB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.2.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\prajubed\\appdata\\local\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.2.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "                                              0.0/95.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 95.2/95.2 kB 5.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: chroma-hnswlib, pypika\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): started\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): finished with status 'error'\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53835 sha256=c59b95fddc9792defdcec362b9eb4338710ae9d3b0d4406fd30fce9dc975e854\n",
      "  Stored in directory: c:\\users\\prajubed\\appdata\\local\\pip\\cache\\wheels\\a3\\01\\bd\\4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Failed to build chroma-hnswlib\n"
     ]
    }
   ],
   "source": [
    " ! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "469190ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5dfa2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "104f8f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d41ee60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=embedding,\n",
    "#     persist_directory=persist_directory\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934eaf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4551cb86",
   "metadata": {},
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "234473c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"is there an email i can ask for help\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2c5c0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "400601e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a5add4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69315596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991db476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c75d1f4f",
   "metadata": {},
   "source": [
    "## Vectorstore retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a5c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
